"""
Jira Application Ticket Generator

Generates application data issues with ML-based diversity
"""

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
import warnings
import platform
warnings.filterwarnings('ignore')

# Check if running on Apple Silicon
def is_apple_silicon():
    return platform.system() == 'Darwin' and platform.processor() == 'arm'

# Setup device
try:
    import torch
    print(f"‚úÖ PyTorch version: {torch.__version__}")
    
    if is_apple_silicon():
        if torch.backends.mps.is_available():
            print("‚úÖ Apple Silicon (M2) GPU detected!")
            print("‚úÖ MPS (Metal Performance Shaders) available")
            DEVICE = "mps"
        else:
            print("‚ö†Ô∏è  MPS not available, using CPU")
            DEVICE = "cpu"
    else:
        DEVICE = "cpu"
    
    print(f"üñ•Ô∏è  Using device: {DEVICE}")
    
except ImportError:
    print("Installing PyTorch for Apple Silicon...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'torch', 'torchvision', '--quiet'])
    import torch
    DEVICE = "mps" if torch.backends.mps.is_available() else "cpu"

try:
    from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM
    print("‚úÖ Transformers library found!")
except ImportError:
    print("Installing transformers...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'transformers', '--quiet'])
    from transformers import pipeline, set_seed, AutoTokenizer, AutoModelForCausalLM
    print("‚úÖ Transformers installed!")

np.random.seed(43)
random.seed(43)
set_seed(43)

class M2OptimizedJiraGenerator:
    def __init__(self, use_gpt=True, batch_size=100):
        """
        M2 GPU-optimized Jira ticket generator
        
        Parameters:
        -----------
        use_gpt : bool
            Use GPT-2 for generation
        batch_size : int
            Batch size for GPU (100 optimal for M2)
        """
        self.use_gpt = use_gpt
        self.batch_size = batch_size
        self.device = DEVICE
        self.description_cache = []
        
        if self.use_gpt:
            print(f"\nü§ñ Loading GPT-2 model on {self.device.upper()}...")
            
            if self.device == "mps":
                print("   Using Apple Silicon GPU acceleration...")
                
                model_name = 'gpt2'
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16
                )
                self.model.to(self.device)
                self.model.eval()
                
                print("‚úÖ Model loaded on Apple Silicon GPU with FP16 precision!")
            else:
                self.generator = pipeline(
                    'text-generation',
                    model='gpt2',
                    device=-1
                )
                print("‚úÖ Model loaded on CPU")
        
        # Application-specific vocabulary
        self.features = [
            'user registration', 'payment processing', 'customer profile',
            'order checkout', 'product search', 'mobile app sync',
            'email notifications', 'shopping cart', 'user authentication',
            'report generation', 'file upload', 'data export',
            'invoice creation', 'subscription management', 'password reset',
            'account settings', 'product catalog', 'wishlist feature',
            'review submission', 'address validation', 'coupon application'
        ]
        
        self.data_issues = [
            'form validation failures', 'data persistence errors',
            'null value handling', 'duplicate record creation',
            'incorrect calculations', 'field validation gaps',
            'data type mismatches', 'text truncation',
            'missing constraints', 'referential integrity'
        ]
        
        self.keywords = [
            'data quality', 'incorrect data', 'validation errors',
            'data integrity', 'inadequate data', 'data requirements',
            'data inconsistency', 'adjustments', 'data risk',
            'missed sla', 'deficiencies', 'data completeness'
        ]
        
        # Jira configurations
        self.issue_types = ['Story', 'Task', 'Bug', 'Epic', 'Sub-task']
        self.priorities_labels = ['Lowest', 'Low', 'Medium', 'High', 'Highest']
        self.priority_map = {'Lowest': 1, 'Low': 2, 'Medium': 3, 'High': 4, 'Highest': 5}
        self.statuses = ['To Do', 'In Progress', 'Code Review', 'Testing', 'Done', 'Closed', 'Blocked']
        self.components = [
            'User Management', 'Payment Processing', 'Order System',
            'Customer Portal', 'Mobile App', 'API Services', 
            'Analytics', 'Reporting', 'Frontend', 'Backend'
        ]
        self.sprints = [f"Sprint {i}" for i in range(1, 21)] + ['Backlog']
    
    def generate_batch_gpu(self, prompts):
        """Generate text batch using M2 GPU"""
        if self.device != "mps":
            results = []
            for prompt in prompts:
                result = self.generator(
                    prompt,
                    max_length=100,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=50256
                )
                results.append(result[0]['generated_text'])
            return results
        
        try:
            inputs = self.tokenizer(
                prompts,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=50
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=100,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    top_p=0.9
                )
            
            generated_texts = self.tokenizer.batch_decode(
                outputs,
                skip_special_tokens=True
            )
            
            return generated_texts
            
        except Exception as e:
            print(f"‚ö†Ô∏è  GPU generation failed: {e}")
            return [f"Development team investigating issue." for _ in prompts]
    
    def pre_generate_descriptions(self, num_defects, num_normal):
        """Pre-generate descriptions using GPU batching"""
        if not self.use_gpt:
            return
        
        print(f"\nüìù Pre-generating {num_defects + num_normal} descriptions using {self.device.upper()}...")
        
        # Generate defect prompts
        defect_prompts = []
        for i in range(num_defects):
            feature = random.choice(self.features)
            issue = random.choice(self.data_issues)
            keyword = random.choice(self.keywords)
            prompt = (
                f"Software bug: {feature} showing {issue}. "
                f"Testing reveals {keyword} affecting users."
            )
            defect_prompts.append(prompt)
        
        # Generate normal prompts
        normal_prompts = []
        for i in range(num_normal):
            feature = random.choice(self.features)
            issues = ['performance issues', 'UI problems', 'timeout errors']
            issue = random.choice(issues)
            prompt = f"Application issue: {feature} experiencing {issue}. Team reviewing."
            normal_prompts.append(prompt)
        
        all_prompts = defect_prompts + normal_prompts
        
        # Process in batches
        for i in range(0, len(all_prompts), self.batch_size):
            batch = all_prompts[i:i+self.batch_size]
            
            if i % 1000 == 0:
                print(f"   Progress: {i}/{len(all_prompts)} ({i/len(all_prompts)*100:.1f}%)")
            
            generated_texts = self.generate_batch_gpu(batch)
            
            for j, generated in enumerate(generated_texts):
                original_prompt = batch[j]
                cleaned = generated.replace(original_prompt, '').strip()
                
                sentences = cleaned.split('.')
                if len(sentences) > 1:
                    cleaned = '. '.join(sentences[:2]) + '.'
                
                if not cleaned or len(cleaned) < 20:
                    cleaned = "Investigation in progress. Development team working on resolution."
                
                self.description_cache.append(cleaned)
        
        print(f"‚úÖ Generated {len(self.description_cache)} unique descriptions")
        random.shuffle(self.description_cache)
    
    def get_cached_description(self):
        """Get pre-generated description"""
        if self.description_cache:
            return self.description_cache.pop()
        return "Team investigating. Fix planned for next release."
    
    def generate_data_defect_issue(self):
        """Generate application data defect"""
        feature = random.choice(self.features)
        issue = random.choice(self.data_issues)
        keyword = random.choice(self.keywords)
        
        problem = f"{feature.title()} experiencing {issue}"
        
        gpt_text = self.get_cached_description()
        
        description = (
            f"QA testing identified {keyword} affecting {feature}. "
            f"{issue.capitalize()} discovered during user acceptance testing. "
            f"{gpt_text} Development team prioritizing fix."
        )
        
        cause = f"Application logic in {feature} missing {keyword} enforcement"
        
        return problem, description, cause
    
    def generate_normal_issue(self):
        """Generate normal application issue"""
        feature = random.choice(self.features)
        issues = [
            'performance slowdown', 'UI rendering issues',
            'timeout errors', 'browser compatibility', 
            'mobile responsiveness', 'loading delays'
        ]
        issue = random.choice(issues)
        
        problem = f"{feature.title()} showing {issue}"
        
        gpt_text = self.get_cached_description()
        
        description = f"Users reporting {issue} in {feature}. {gpt_text}"
        
        cause = f"Frontend optimization needed for {feature}"
        
        return problem, description, cause
    
    def generate_dataset(self, num_issues=10000, defect_ratio=0.32):
        """Generate Jira dataset with M2 GPU acceleration"""
        print(f"\n{'='*80}")
        print(f"M2 GPU-ACCELERATED JIRA GENERATION: {num_issues} ISSUES")
        print('='*80)
        
        num_defects = int(num_issues * defect_ratio)
        num_normal = num_issues - num_defects
        
        # Pre-generate with GPU
        if self.use_gpt:
            self.pre_generate_descriptions(num_defects, num_normal)
        
        # Assemble issues
        print(f"\nüé´ Assembling {num_issues} Jira issues...")
        
        issues = []
        defect_indices = set(random.sample(range(num_issues), num_defects))
        
        epic_keys = []
        story_keys = []
        
        for i in range(num_issues):
            if i % 2000 == 0 and i > 0:
                print(f"   Assembled: {i}/{num_issues}")
            
            project = random.choice(['APP', 'WEB', 'MOB', 'API', 'USR'])
            issue_key = f"{project}-{10000 + i}"
            
            created = datetime.now() - timedelta(days=random.randint(1, 120))
            updated = created + timedelta(hours=random.randint(1, 720))
            
            issue_type = np.random.choice(self.issue_types, p=[0.35, 0.25, 0.25, 0.05, 0.10])
            priority_label = np.random.choice(self.priorities_labels, p=[0.10, 0.25, 0.35, 0.20, 0.10])
            
            status = np.random.choice(self.statuses, p=[0.15, 0.25, 0.15, 0.15, 0.15, 0.10, 0.05])
            resolution = 'Done' if status in ['Done', 'Closed'] else 'Unresolved'
            
            is_defect = i in defect_indices
            
            if is_defect:
                problem, description, cause = self.generate_data_defect_issue()
                component = random.choice(['User Management', 'Payment Processing', 'Order System', 'Customer Portal'])
            else:
                problem, description, cause = self.generate_normal_issue()
                component = random.choice(self.components)
            
            parent_epic_id = ''
            parent_story_id = ''
            has_child_stories = 0
            has_child_subtasks = 0
            
            if issue_type == 'Epic':
                epic_keys.append(issue_key)
            elif issue_type == 'Story':
                story_keys.append(issue_key)
                if epic_keys and random.random() < 0.6:
                    parent_epic_id = random.choice(epic_keys)
            elif issue_type == 'Sub-task':
                if story_keys and random.random() < 0.7:
                    parent_story_id = random.choice(story_keys)
            
            issues.append({
                'issue_key': issue_key,
                'issue_type': issue_type,
                'priority_numeric': self.priority_map[priority_label],
                'status': status,
                'component': component,
                'story_points': np.random.choice([1, 2, 3, 5, 8, 13, 21], p=[0.15, 0.25, 0.25, 0.2, 0.1, 0.04, 0.01]),
                'sprint': random.choice(self.sprints),
                'assignee_id': random.randint(1, 200),
                'reporter_id': random.randint(1, 200),
                'resolution': resolution,
                'num_comments': int(np.clip(np.random.poisson(4), 0, 50)),
                'num_attachments': np.random.choice([0, 1, 2, 3, 4, 5], p=[0.5, 0.25, 0.15, 0.06, 0.03, 0.01]),
                'watchers_count': int(np.clip(np.random.poisson(2), 0, 20)),
                'time_to_resolution_hours': round(max(1, np.random.normal(72, 20)), 2),
                'num_transitions': np.random.choice([1, 2, 3, 4, 5, 6], p=[0.3, 0.3, 0.2, 0.1, 0.07, 0.03]),
                'description_length': len(description.split()),
                'ticket_age_days': (datetime.now() - created).days,
                'has_labels': np.random.choice([0, 1], p=[0.4, 0.6]),
                'parent_epic_id': parent_epic_id,
                'parent_story_id': parent_story_id,
                'has_child_stories': has_child_stories,
                'has_child_subtasks': has_child_subtasks,
                'priority': priority_label,
                'created_date': created.strftime('%Y-%m-%d'),
                'updated_date': updated.strftime('%Y-%m-%d'),
                'problem_statement': problem,
                'description': description,
                'cause': cause
            })
        
        print(f"   Assembled: {num_issues}/{num_issues} ‚úì")
        
        df = pd.DataFrame(issues)
        
        # Update hierarchy flags
        for idx, row in df.iterrows():
            if row['issue_type'] == 'Epic':
                if len(df[df['parent_epic_id'] == row['issue_key']]) > 0:
                    df.at[idx, 'has_child_stories'] = 1
            elif row['issue_type'] == 'Story':
                if len(df[df['parent_story_id'] == row['issue_key']]) > 0:
                    df.at[idx, 'has_child_subtasks'] = 1
        
        df = df.sample(frac=1, random_state=43).reset_index(drop=True)
        
        return df

def main():
    print("="*80)
    print("M2 GPU-ACCELERATED JIRA TICKET GENERATOR")
    print("="*80)
    
    # Configuration
    USE_GPT = True
    BATCH_SIZE = 100
    
    if USE_GPT and DEVICE == "mps":
        print("\n‚ö° MODE: Apple Silicon M2 GPU Acceleration")
        print("   Batch size: 100 (optimized for M2)")
        print("   Precision: FP16 (faster inference)")
        print("   Expected time: 30-60 seconds for 10,000 issues!")
    elif USE_GPT:
        print("\n‚ö° MODE: CPU-based generation")
        print("   Expected time: 1-2 minutes")
    else:
        print("\n‚ö° MODE: Template-based (No ML)")
        print("   Expected time: 10 seconds")
    
    import time
    start_time = time.time()
    
    generator = M2OptimizedJiraGenerator(
        use_gpt=USE_GPT,
        batch_size=BATCH_SIZE
    )
    
    # Generate 10,000 issues (change to 1000 for faster testing)
    df = generator.generate_dataset(num_issues=10000, defect_ratio=0.32)
    
    elapsed_time = time.time() - start_time
    
    # Save
    output_file = 'jira_m2_accelerated.csv'
    df.to_csv(output_file, index=False)
    
    # Statistics
    print(f"\n{'='*80}")
    print("GENERATION COMPLETE")
    print('='*80)
    print(f"‚úÖ Total issues: {len(df)}")
    print(f"‚úÖ Unique descriptions: {len(df['description'].unique())}")
    print(f"‚úÖ Diversity: {len(df['description'].unique())/len(df)*100:.1f}%")
    print(f"‚è±Ô∏è  Time: {elapsed_time:.1f}s ({elapsed_time/60:.1f} min)")
    print(f"‚ö° Speed: {len(df)/elapsed_time:.0f} issues/sec")
    print(f"üéØ Device: {DEVICE.upper()}")
    print(f"üìÅ Saved: {output_file}")
    
    # Issue type breakdown
    print(f"\nüìä Issue Type Distribution:")
    print(df['issue_type'].value_counts())
    
    # Speedup calculation
    if DEVICE == "mps":
        cpu_estimated = elapsed_time * 3
        print(f"\nüí° GPU Speedup: ~3x faster (CPU would take ~{cpu_estimated:.1f}s)")
    
    # Sample output
    print(f"\n{'='*80}")
    print("SAMPLE JIRA ISSUES")
    print('='*80)
    for _, row in df.sample(3).iterrows():
        print(f"\nüé´ Issue: {row['issue_key']}")
        print(f"Type: {row['issue_type']} | Priority: {row['priority']} | Status: {row['status']}")
        print(f"Problem: {row['problem_statement']}")
        print(f"Description: {row['description'][:150]}...")
    
    return df

if __name__ == '__main__':
    df = main()
